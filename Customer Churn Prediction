!pip install scikit-learn pandas

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
try:
    df = pd.read_csv('customer_data.csv')
except FileNotFoundError:
    print("Error: 'customer_data.csv' not found. Please upload the file or provide the correct path.")
    # Create a dummy dataframe for demonstration if the file is not found
    data = {
        'customer_id': range(100),
        'age': np.random.randint(20, 60, 100),
        'monthly_charges': np.random.uniform(20, 100, 100),
        'total_charges': np.random.uniform(100, 2000, 100),
        'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], 100),
        'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], 100),
        'usage_minutes': np.random.randint(50, 1000, 100),
        'Churn': np.random.randint(0, 2, 100) # 1 for churn, 0 for no churn
    }
    df = pd.DataFrame(data)
    print("Created a dummy dataset for demonstration.")

# Preprocessing: Handle categorical features and potential missing values
# For simplicity, we'll use one-hot encoding for categorical features
df = pd.get_dummies(df, columns=['contract_type', 'internet_service'], drop_first=True)

# Assume 'total_charges' might have missing values (e.g., for new customers)
# We'll fill missing values with the mean for simplicity
df['total_charges'] = df['total_charges'].fillna(df['total_charges'].mean())

# Define features (X) and target (y)
# Exclude customer_id and the original categorical columns
X = df.drop(['customer_id', 'Churn'], axis=1)
y = df['Churn']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train and evaluate Logistic Regression model
print("Training Logistic Regression...")
log_reg = LogisticRegression(solver='liblinear') # Use a solver that works well with smaller datasets
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)

print("\nLogistic Regression Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("Classification Report:\n", classification_report(y_test, y_pred_log_reg))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_log_reg))

# Train and evaluate Random Forest model
print("\nTraining Random Forest...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("\nRandom Forest Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# Train and evaluate Gradient Boosting model
print("\nTraining Gradient Boosting...")
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

print("\nGradient Boosting Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print("Classification Report:\n", classification_report(y_test, y_pred_gb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gb))

